{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import keras\n",
    "from keras import models\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "# from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, \\\n",
    "    AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout, GlobalAveragePooling2D, Resizing, LeakyReLU\n",
    "from keras.applications.efficientnet import EfficientNetB0\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import initializers, layers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtcnn(mtcnn_dir, training_dir):\n",
    "\n",
    "    def affineMatrix(lmks, scale=2.5):\n",
    "        nose = np.array(lmks['nose'], dtype=np.float32)\n",
    "        left_eye = np.array(lmks['left_eye'], dtype=np.float32)\n",
    "        right_eye = np.array(lmks['right_eye'], dtype=np.float32)\n",
    "        eye_width = right_eye - left_eye\n",
    "        angle = np.arctan2(eye_width[1], eye_width[0])\n",
    "        center = nose\n",
    "        alpha = np.cos(angle)\n",
    "        beta = np.sin(angle)\n",
    "        w = np.sqrt(np.sum(eye_width ** 2)) * scale\n",
    "        m = [[alpha, beta, -alpha * center[0] - beta * center[1] + w * 0.5],\n",
    "             [-beta, alpha, beta * center[0] - alpha * center[1] + w * 0.5]]\n",
    "        return np.array(m), (int(w), int(w))  # ï¼ˆaffine matrix, target size)\n",
    "\n",
    "    detector = MTCNN(steps_threshold=[0.0, 0.0, 0.0])\n",
    "    for i in training_dir:\n",
    "        print(\"processing \", i)\n",
    "        img = cv.imread(training_dir + i)\n",
    "        faces = detector.detect_faces(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n",
    "        face = max(faces, key=lambda x: x['confidence'])\n",
    "        mat, size = affineMatrix(face['keypoints'])\n",
    "\n",
    "        tmp = cv.warpAffine(img, mat, size)\n",
    "        cv.imwrite(mtcnn_dir + i, tmp)\n",
    "\n",
    "        img = Image.open(mtcnn_dir + i)\n",
    "        new_img = img.resize((256, 256))\n",
    "        new_img.save(mtcnn_dir + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(deepfake_dir, face2face_dir, real_dir, training_dir, Re_save=False, mt_cnn=False):\n",
    "    deepfake_img = os.listdir(deepfake_dir)\n",
    "    face2face_img = os.listdir(face2face_dir)\n",
    "    real_img = os.listdir(real_dir)\n",
    "\n",
    "    if Re_save:\n",
    "        if not os.path.exists(training_dir):\n",
    "            os.makedirs(training_dir)\n",
    "\n",
    "        for filename in deepfake_img:\n",
    "            print(\"Deepfake\")\n",
    "            original = deepfake_dir + filename\n",
    "            target = training_dir + 'deepfake_' + filename\n",
    "            shutil.copyfile(original, target)\n",
    "\n",
    "        for filename in face2face_img:\n",
    "            print(\"Face2Face\")\n",
    "            original = face2face_dir + filename\n",
    "            target = training_dir + 'face2face_' + filename\n",
    "            shutil.copyfile(original, target)\n",
    "\n",
    "        for filename in real_img:\n",
    "            print(\"Real\")\n",
    "            original = real_dir + filename\n",
    "            target = training_dir + 'real_' + filename\n",
    "            shutil.copyfile(original, target)\n",
    "\n",
    "    if mt_cnn:\n",
    "        mtcnn_dir = './testing/'\n",
    "        mtcnn(mtcnn_dir, training_dir)\n",
    "        training_img = os.listdir(\"./testing/\")\n",
    "    else:\n",
    "        training_img = os.listdir(\"./training_data/\")\n",
    "\n",
    "    categories = []\n",
    "    for i in training_img:\n",
    "        tmp = i.split(\"_\")[0]\n",
    "        if tmp == \"deepfake\":\n",
    "            categories.append(\"fake\")\n",
    "        elif tmp == \"face2face\":\n",
    "            categories.append(\"fake\")\n",
    "        else:\n",
    "            categories.append(\"real\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'filename': training_img,\n",
    "        'category': categories\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_set(df, batch_size):\n",
    "    df[\"category\"] = df[\"category\"].replace({0: 'fake', 1: 'real'})\n",
    "    train_df, validate_df = train_test_split(df, test_size=0.20, random_state=None, stratify=df[\"category\"])\n",
    "\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    validate_df = validate_df.reset_index(drop=True)\n",
    "\n",
    "    total_train = train_df.shape[0]\n",
    "    total_validate = validate_df.shape[0]\n",
    "\n",
    "    # Training Set\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        train_df,\n",
    "        \"./training_data/\",\n",
    "        x_col='filename',\n",
    "        y_col='category',\n",
    "        target_size=IMAGE_SIZE,  # Default : 299, 299\n",
    "        class_mode='binary',\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    print(train_generator.class_indices)\n",
    "\n",
    "    # =======================================================\n",
    "    # Validation Set\n",
    "    validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    validation_generator = validation_datagen.flow_from_dataframe(\n",
    "        validate_df,\n",
    "        \"./training_data/\",\n",
    "        x_col='filename',\n",
    "        y_col='category',\n",
    "        target_size=IMAGE_SIZE,\n",
    "        class_mode='binary',\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    print(validation_generator.class_indices)\n",
    "\n",
    "    return train_generator, validation_generator, total_train, total_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, train_data, validate_data, batch_size, num_train, num_validation):\n",
    "    # Prevent the model being overfit\n",
    "    earlystop = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "    # Improve the learning rate\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                                patience=2,\n",
    "                                                verbose=1,\n",
    "                                                mode='auto')\n",
    "                                                # factor=0.5,\n",
    "                                                # min_lr=0.00001)\n",
    "\n",
    "    callbacks = [earlystop, learning_rate_reduction]\n",
    "\n",
    "    total_train = num_train\n",
    "    total_validate = num_validation\n",
    "    batch_size = batch_size\n",
    "\n",
    "    if TRAIN:\n",
    "        epochs = 3 if FAST_RUN else 30\n",
    "\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            epochs=epochs,\n",
    "            validation_data=validate_data,\n",
    "            validation_steps=total_validate // batch_size,\n",
    "            steps_per_epoch=total_train // batch_size,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        model.save_weights(\"./testing.h5\")\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "        ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "        ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n",
    "        ax1.set_xticks(np.arange(1, epochs, 1))\n",
    "        ax1.set_yticks(np.arange(0, 1, 0.1))\n",
    "        legend = ax1.legend(loc='best', shadow=True)\n",
    "\n",
    "        ax2.plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "        ax2.plot(history.history['val_accuracy'], color='r', label=\"Validation accuracy\")\n",
    "        ax2.set_xticks(np.arange(1, epochs, 1))\n",
    "\n",
    "        legend = ax2.legend(loc='best', shadow=True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./test.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_EfficientNet():\n",
    "    model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "    print('Model loaded.')\n",
    "\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()  # Determines the type of top model\n",
    "\n",
    "    top_model.add(model)\n",
    "\n",
    "    # formats the input for the classifier to the convolutional base output\n",
    "    top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "\n",
    "    # Condenses the input from flatten down to 256 nodes\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    top_model.add(Dropout(0.5))\n",
    "\n",
    "    # Condenses the remaining input down into 2 categories\n",
    "    top_model.add(Dense(1, activation='softmax', name='predictions'))\n",
    "\n",
    "    top_model.compile(loss=loss_function,\n",
    "                      optimizer=optimizers.SGD(lr=learning_rate, momentum=momentum, decay=1e-6, nesterov=False),\n",
    "                      # learning rate(lr) and momentum are Core Variables\n",
    "                      # SGD options to consider: Nesterov, learning rate decay\n",
    "                      metrics=[metrics])\n",
    "\n",
    "    # top_model.compile(loss=loss_function, optimizer='Adam', metrics=['accuracy'])\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_VGG16():\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "    # model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "    print('Model loaded.')\n",
    "\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()  # Determines the type of top model\n",
    "\n",
    "    top_model.add(model)\n",
    "\n",
    "    # formats the input for the classifier to the convolutional base output\n",
    "    top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "\n",
    "    # Condenses the input from flatten down to 256 nodes\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    top_model.add(Dropout(0.5))\n",
    "\n",
    "    # Condenses the remaining input down into 2 categories\n",
    "    top_model.add(Dense(1, activation='softmax', name='predictions'))\n",
    "\n",
    "    top_model.compile(loss=loss_function,\n",
    "                      optimizer=optimizers.SGD(lr=learning_rate, momentum=momentum, decay=1e-6, nesterov=False),\n",
    "                      # learning rate(lr) and momentum are Core Variables\n",
    "                      # SGD options to consider: Nesterov, learning rate decay\n",
    "                      metrics=[metrics])\n",
    "\n",
    "    # top_model.compile(loss=loss_function, optimizer='Adam', metrics=['accuracy'])\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ResNet50():\n",
    "    model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "    print('Model loaded.')\n",
    "\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()  # Determines the type of top model\n",
    "\n",
    "    top_model.add(model)\n",
    "\n",
    "    # formats the input for the classifier to the convolutional base output\n",
    "    top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "\n",
    "    # Condenses the input from flatten down to 256 nodes\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    top_model.add(Dropout(0.5))\n",
    "\n",
    "    top_model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    # Condenses the remaining input down into 2 categories\n",
    "    top_model.add(Dense(1, activation='softmax', name='predictions'))\n",
    "\n",
    "    top_model.compile(loss=loss_function,\n",
    "                      optimizer=optimizers.SGD(lr=learning_rate, momentum=momentum, decay=1e-6, nesterov=False),\n",
    "                      # learning rate(lr) and momentum are Core Variables\n",
    "                      # SGD options to consider: Nesterov, learning rate decay\n",
    "                      metrics=[metrics])\n",
    "\n",
    "    # top_model.compile(loss=loss_function, optimizer='Adam', metrics=['accuracy'])\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mesonet():\n",
    "    x = Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS))\n",
    "    x1 = Resizing(224, 224, interpolation='bilinear', crop_to_aspect_ratio=False)(x)\n",
    "    x1 = Conv2D(8, (3, 3), padding='same', activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "\n",
    "    x2 = Conv2D(8, (5, 5), padding='same', activation='relu')(x1)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n",
    "\n",
    "    x3 = Conv2D(16, (5, 5), padding='same', activation='relu')(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "\n",
    "    x4 = Conv2D(16, (5, 5), padding='same', activation='relu')(x3)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "\n",
    "    y = Flatten()(x4)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(16)(y)\n",
    "    y = LeakyReLU(alpha=0.1)(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0002), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 297, 297, 256)     7168      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 297, 297, 256)    1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 148, 148, 256)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 148, 148, 256)     0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 146, 146, 512)     1180160   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 146, 146, 512)    2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 73, 73, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 73, 73, 512)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 71, 71, 256)       1179904   \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 71, 71, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 35, 35, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 35, 35, 256)       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 33, 33, 128)       295040    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 33, 33, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 14, 14, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 14, 14, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 7, 7, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 25090     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               384       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,988,675\n",
      "Trainable params: 2,985,859\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n",
      "Found 9600 validated image filenames belonging to 2 classes.\n",
      "{'fake': 0, 'real': 1}\n",
      "Found 2400 validated image filenames belonging to 2 classes.\n",
      "{'fake': 0, 'real': 1}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[20,512,146,146] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/conv2d_5/Relu\n (defined at C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\backend.py:4867)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7691]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential/conv2d_5/Relu:\nIn[0] sequential/conv2d_5/BiasAdd (defined at C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\layers\\convolutional.py:264)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\Marco\\AppData\\Local\\Temp/ipykernel_4672/157768398.py\", line 35, in <module>\n>>>     model_training(model, train_set, validation_set, batch_size, num_train, num_validation)\n>>> \n>>>   File \"C:\\Users\\Marco\\AppData\\Local\\Temp/ipykernel_4672/3407511316.py\", line 25, in model_training\n>>>     history = model.fit(\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\sequential.py\", line 373, in call\n>>>     return super(Sequential, self).call(inputs, training=training, mask=mask)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 273, in call\n>>>     return self.activation(outputs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\activations.py\", line 311, in relu\n>>>     return backend.relu(x, alpha=alpha, max_value=max_value, threshold=threshold)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\backend.py\", line 4867, in relu\n>>>     x = tf.nn.relu(x)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4672/157768398.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_data_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mmodel_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4672/3407511316.py\u001b[0m in \u001b[0;36mmodel_training\u001b[1;34m(model, train_data, validate_data, batch_size, num_train, num_validation)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mFAST_RUN\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         history = model.fit(\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[20,512,146,146] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/conv2d_5/Relu\n (defined at C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\backend.py:4867)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7691]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential/conv2d_5/Relu:\nIn[0] sequential/conv2d_5/BiasAdd (defined at C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\layers\\convolutional.py:264)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\asyncio\\events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\Marco\\AppData\\Local\\Temp/ipykernel_4672/157768398.py\", line 35, in <module>\n>>>     model_training(model, train_set, validation_set, batch_size, num_train, num_validation)\n>>> \n>>>   File \"C:\\Users\\Marco\\AppData\\Local\\Temp/ipykernel_4672/3407511316.py\", line 25, in model_training\n>>>     history = model.fit(\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\sequential.py\", line 373, in call\n>>>     return super(Sequential, self).call(inputs, training=training, mask=mask)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 273, in call\n>>>     return self.activation(outputs)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\activations.py\", line 311, in relu\n>>>     return backend.relu(x, alpha=alpha, max_value=max_value, threshold=threshold)\n>>> \n>>>   File \"C:\\Users\\Marco\\anaconda3\\envs\\ML_tensorflow\\lib\\site-packages\\keras\\backend.py\", line 4867, in relu\n>>>     x = tf.nn.relu(x)\n>>> "
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN = True\n",
    "Re_save_data = False\n",
    "use_mt_cnn = False\n",
    "FAST_RUN = False\n",
    "batch_size = 20\n",
    "loss_function = 'categorical_crossentropy'\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "metrics = 'accuracy'\n",
    "\n",
    "IMAGE_WIDTH = 299\n",
    "IMAGE_HEIGHT = 299\n",
    "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS = 3  # R, G, B\n",
    "\n",
    "deepfake_dir = \"./fake_deepfake/\"\n",
    "face2face_dir = \"./fake_face2face/\"\n",
    "real_dir = \"./real/\"\n",
    "training_dir = \"./training_data/\"\n",
    "\n",
    "\n",
    "# model = create_VGG16()\n",
    "# model = create_ResNet50()\n",
    "# model = create_EfficientNet()\n",
    "# model = create_mesonet()\n",
    "# model = test_model()\n",
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "df = preprocessing(deepfake_dir, face2face_dir, real_dir, training_dir, Re_save_data, use_mt_cnn)\n",
    "\n",
    "train_set, validation_set, num_train, num_validation = generate_data_set(df, batch_size)\n",
    "\n",
    "model_training(model, train_set, validation_set, batch_size, num_train, num_validation)\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
